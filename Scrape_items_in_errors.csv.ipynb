{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, re, pickle, argparse\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_filepath = 'Minutes/raw/errors.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "selenium_filepath = \"C:\\GIT\\SELENIUM_DRIVERS\\chromedriver_win32\\chromedriver.exe\"\n",
    "save_root_dir = './Minutes'\n",
    "\n",
    "url = \"https://www.federalreserve.gov/monetarypolicy/materials/\"\n",
    "\n",
    "def prepare_resources_for_scraping(selenium_filepath, url, start_mmddyyyy, end_mmddyyyy, scrape_target='minutes'):\n",
    "    driver = webdriver.Chrome(selenium_filepath)\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # set start date\n",
    "    start_date = driver.find_element_by_name(\"startmodel\")\n",
    "    start_date.clear()\n",
    "    start_date.send_keys(start_mmddyyyy)\n",
    "\n",
    "    # set end date\n",
    "    end_date = driver.find_element_by_name(\"endmodel\")\n",
    "    end_date.clear()\n",
    "    end_date.send_keys(end_mmddyyyy)\n",
    "\n",
    "    # select items\n",
    "    if scrape_target == 'minutes':\n",
    "        xpath_strings = \"//label/input[contains(..,'Minutes (1993-Present)')]\"\n",
    "    elif scrape_target == 'statements':\n",
    "        xpath_strings = \"//label/input[contains(..,'Policy Statements')]\"\n",
    "    statement_checkbox = driver.find_element_by_xpath(xpath_strings)\n",
    "    statement_checkbox.click()\n",
    "\n",
    "    # apply filter\n",
    "    submit = driver.find_element_by_css_selector(\".btn.btn-primary\")\n",
    "    submit.click()\n",
    "    \n",
    "    # get the page control row\n",
    "    pagination = driver.find_element_by_class_name('pagination')\n",
    "\n",
    "    # go to the last page to find the largest page number\n",
    "    last_page = pagination.find_element_by_link_text('Last')\n",
    "    last_page.click()\n",
    "    pages = pagination.text.split('\\n')\n",
    "    largest_page = int(pages[-3])\n",
    "    \n",
    "    return driver, pagination, largest_page\n",
    "\n",
    "def scrape_URLs_and_meeting_dates_and_document_dates(driver, pagination, largest_page, scrape_target='minutes'):\n",
    "    statement_url_list, meeting_date_list, document_date_list = [], [], []\n",
    "    # go back to first page and start the loop\n",
    "    first_page = pagination.find_element_by_link_text('First')\n",
    "    first_page.click()\n",
    "    next_page = pagination.find_element_by_link_text('Next')\n",
    "    for i in range(largest_page):\n",
    "        # now to get the items inside\n",
    "        main = driver.find_element_by_css_selector(\".panel.panel-default\") # get the app panel\n",
    "        material_types = main.find_elements_by_css_selector(\".fomc-meeting__month.col-xs-5.col-sm-3.col-md-4\") # get the 2nd col\n",
    "        material_types = [element.text for element in material_types] # to get the words\n",
    "        material_links = main.find_elements_by_css_selector(\".fomc-meeting__month.col-xs-5.col-sm-3.col-md-2\") # get the 3rd col\n",
    "        \n",
    "        meeting_dates = main.find_elements_by_css_selector(\".fomc-meeting__month.col-xs-5.col-sm-3.col-md-3 > strong\")\n",
    "        meeting_dates = [element.text for element in meeting_dates] # to get the words\n",
    "        meeting_dates = meeting_dates[2:] # First two items correspond to table headings (i.e.,\"Meeting date\", \"Document date\")\n",
    "        \n",
    "        document_dates = main.find_elements_by_css_selector(\".fomc-meeting__month.col-xs-5.col-sm-3.col-md-3 > em\")\n",
    "        document_dates = [element.text for element in document_dates] # to get the words\n",
    "        \n",
    "        # add url to statement_url_list if it is a target item\n",
    "        if scrape_target == 'minutes':\n",
    "            target_strings = 'Minutes'\n",
    "            html_elements = []\n",
    "            for element in material_links:\n",
    "                try: html_elements.append(element.find_element_by_link_text('HTML'))\n",
    "                except: continue\n",
    "        elif scrape_target == 'statements':\n",
    "            target_strings = 'Statement'\n",
    "            html_elements = [element.find_element_by_link_text('HTML') for element in material_links] # get the html ones\n",
    "        \n",
    "        meeting_date_list.extend([meeting_dates[i] for i, j in enumerate(material_types) if j==target_strings])\n",
    "        statement_url_list.extend([html_elements[i].get_attribute('href') for i, j in enumerate(material_types) if j==target_strings])\n",
    "        document_date_list.extend([document_dates[i] for i, j in enumerate(material_types) if j==target_strings])\n",
    "        \n",
    "        next_page.click()\n",
    "    print('Number of URLs: {}'.format(len(statement_url_list)))\n",
    "    \n",
    "    return statement_url_list, meeting_date_list, document_date_list\n",
    "\n",
    "def get_text_for_a_statement_from_201201_to_202209(soup):\n",
    "    return soup.find('div', class_ = 'col-xs-12 col-sm-8 col-md-9').text.strip()\n",
    "\n",
    "def get_text_for_a_statement_from_200710_to_201112(soup):\n",
    "    return soup.find('div', id=\"leftText\").text.strip()\n",
    "\n",
    "def get_text_for_a_statement_from_199601_to_200709(soup):\n",
    "    return '\\n'.join([item.text.strip() for item in soup.select('table td')])\n",
    "\n",
    "def get_text_for_a_statement_from_199401_to_199512(soup):\n",
    "    return soup.find('div', id=\"content\").text.strip()\n",
    "\n",
    "doublespace_pattern = re.compile('\\s+')\n",
    "def remove_doublespaces(document):\n",
    "    return doublespace_pattern.sub(' ', document).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>meeting_date</th>\n",
       "      <th>document_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.federalreserve.gov/monetarypolicy/...</td>\n",
       "      <td>December 13, 2011</td>\n",
       "      <td>January 3, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.federalreserve.gov/fomc/minutes/20...</td>\n",
       "      <td>September 18, 2007</td>\n",
       "      <td>October 9, 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.federalreserve.gov/fomc/minutes/20...</td>\n",
       "      <td>August 16, 2007</td>\n",
       "      <td>October 9, 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.federalreserve.gov/fomc/minutes/20...</td>\n",
       "      <td>August 10, 2007</td>\n",
       "      <td>October 9, 2007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url        meeting_date  \\\n",
       "0  https://www.federalreserve.gov/monetarypolicy/...   December 13, 2011   \n",
       "1  https://www.federalreserve.gov/fomc/minutes/20...  September 18, 2007   \n",
       "2  https://www.federalreserve.gov/fomc/minutes/20...     August 16, 2007   \n",
       "3  https://www.federalreserve.gov/fomc/minutes/20...     August 10, 2007   \n",
       "\n",
       "     document_date  \n",
       "0  January 3, 2012  \n",
       "1  October 9, 2007  \n",
       "2  October 9, 2007  \n",
       "3  October 9, 2007  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df = pd.read_csv(error_filepath)\n",
    "error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ./Minutes\\raw\\2011\\20111213.txt\n",
      "Saved ./Minutes\\raw\\2007\\20070918.txt\n",
      "Saved ./Minutes\\raw\\2007\\20070816.txt\n",
      "Saved ./Minutes\\raw\\2007\\20070810.txt\n",
      "Saved 254 unique documents under ./Minutes/raw\n"
     ]
    }
   ],
   "source": [
    "for statement_url, meeting_date, document_date in error_df.values:\n",
    "\n",
    "    # Scrape statements\n",
    "    statement_resp = requests.get(statement_url)\n",
    "    statement_soup = BeautifulSoup(statement_resp.content, 'lxml')\n",
    "\n",
    "    document_date_yyyymmdd = datetime.strftime(datetime.strptime(meeting_date, \"%B %d, %Y\"), \"%Y%m%d\")\n",
    "    yearmonth = int(document_date_yyyymmdd[:6])\n",
    "\n",
    "    if yearmonth >= 201201:\n",
    "        doc = get_text_for_a_statement_from_201201_to_202209(statement_soup)\n",
    "    elif yearmonth >= 200710:\n",
    "        doc = get_text_for_a_statement_from_200710_to_201112(statement_soup)\n",
    "    elif yearmonth >= 199601:\n",
    "        doc = get_text_for_a_statement_from_199601_to_200709(statement_soup)    \n",
    "    else:\n",
    "        doc = get_text_for_a_statement_from_199401_to_199512(statement_soup)\n",
    "\n",
    "    # Clean\n",
    "    doc = remove_doublespaces(doc)\n",
    "\n",
    "    # Save data\n",
    "    save_dir = os.path.join(save_root_dir, 'raw', document_date_yyyymmdd[:4])\n",
    "    if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "    save_filepath = os.path.join(save_dir, '{}.txt'.format(document_date_yyyymmdd))\n",
    "    with open(save_filepath, \"w\", encoding='utf-8-sig') as file:\n",
    "        file.write(\"MEETING_DATE: {}\\n\".format(meeting_date))\n",
    "        file.write(doc)\n",
    "        print('Saved {}'.format(save_filepath))\n",
    "\n",
    "save_dir = '{}/{}'.format(save_root_dir, 'raw')\n",
    "print('Saved {} unique documents under {}'.format(len(glob('{}/*/*.txt'.format(save_dir))), save_dir)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
